import fs from 'fs'
import { ContentClient, DeploymentWithMetadataContentAndPointers } from 'dcl-catalyst-client'
import { EntityType } from 'dcl-catalyst-commons'
import { getSnapshot } from './common/Requests';
import { FileHash, EntityId } from "./common/types";
import { executeWithProgressBar, splitIntoChunks } from './common/Helper';
import { clearLogFiles, log } from './common/IO';
import { getGlobalArgParser } from './common/Global';

const THRESHOLD = 15 // 15 MB

async function run() {
    const parser = getGlobalArgParser()
    parser.parseArgs()
    clearLogFiles()

    // This file is generated by executing an 'ls -l' on the /contents folder on the server
    const content = fs.readFileSync('/Users/nchamo/Desktop/file-sizes.txt').toString()
    const entries: [FileHash, number][] = content.split('\n')
        .map(line => line.trim())
        .map(line => {
            const pieces = line.split(' ')
            const size = parseInt(pieces[0]) / 1024 // Size here will be in KB
            const hash = pieces[pieces.length - 1]
            return [hash, size]
        })

    const fileSizes = new Map(entries)

    await runCheck('https://peer-testing.decentraland.org/content', fileSizes)
}

async function runCheck(serverAddress: string, fileSizes: Map<FileHash, number>): Promise<void> {
    // Find snapshot
    const { snapshot } = await getSnapshot(serverAddress, EntityType.SCENE)
    const activeEntityIds: Set<EntityId> = new Set(snapshot.keys())
    console.log(`Found ${activeEntityIds.size} active scenes. Going to fetch them.`)

    // Fetch active entities
    const activeEntities: DeploymentWithMetadataContentAndPointers[] = await fetchEntities(serverAddress, activeEntityIds)
    console.log(`Fetched ${activeEntities.length} active scenes`)

    // Calculate size occupied by each entity
    const referencedEntities: Map<EntityId, number> = calculateSize(activeEntities, fileSizes);

    // Sort entities
    const sortedEntities = Array.from(referencedEntities.entries())
        .sort(([_, size1], [__, size2]) => size2 - size1)
        .map(([entityId, size]) => `${entityId}: ${size}`)
        .join('\n');
    await log(sortedEntities)

    // Report amount of entities over 15 MB
    const scenesOverLimit = Array.from(referencedEntities.entries())
        .filter(([ _, size ]) => size >= THRESHOLD)
        .map(([ sceneId ]) => sceneId)
    console.log(`Found ${scenesOverLimit.length} scenes over 15 MB`)
}

async function fetchEntities(serverAddress: string, ids: Set<EntityId>) {
    const contentClient = new ContentClient(serverAddress, 'catalyst-scripts')
    const chunks: EntityId[][] = splitIntoChunks([...ids], 100)

    const entities: DeploymentWithMetadataContentAndPointers[][] = await executeWithProgressBar('Fetching entities', chunks, (entityIds: EntityId[]) => contentClient.fetchAllDeployments({ filters: { entityIds, onlyCurrentlyPointed: true }}), 5);
    let result: DeploymentWithMetadataContentAndPointers[] = []
    for (const subEntities of entities) {
        result = result.concat(subEntities);
    }
    return result
}

// Calculate MB per parcel occupied by each scene
function calculateSize(entities: DeploymentWithMetadataContentAndPointers[], fileSizes: Map<FileHash, number>): Map<EntityId, number> {
    const result: Map<EntityId, number> = new Map()

    for (const entity of entities) {
        const entitySize = (entity.content ?? []) // Counted in total KB
            .map(({ hash }) => {
                const fileSize = fileSizes.get(hash);
                if (fileSize === undefined) {
                    throw new Error(`Failed to find size for hash ${hash}. Found on entity ${entity.entityId}`)
                }
                return fileSize
            })
            .reduce((accum, curr) => accum + curr, 0)
        result.set(entity.entityId, entitySize / 1024 / entity.pointers.length) // Set in MB per parcel
    }

    return result
}

run().then(() => console.log("Done!"))

